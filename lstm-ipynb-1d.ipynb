{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "lstm.ipynb-1d의 ",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyO+J7/TA6epvtwpYEpwUUe9",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/gist/uquant77/e43117d35ad9ad7c0cc93d973066508c/lstm-ipynb-1d.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Load Python libraries"
      ],
      "metadata": {
        "id": "O3XQb4p3bAoy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# pip install numpy, pandas, pprint\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import pprint\n",
        "# pip install torch\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.autograd import Variable \n",
        "import torch.nn.init as init\n",
        "\n",
        "# pip install matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.pyplot import figure\n",
        "\n",
        "# pip install ccxt\n",
        "!pip install ccxt\n",
        "import ccxt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0NkmbBSsbr8m",
        "outputId": "c46bef4b-e52d-4137-d4c2-2c21092f7e78"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting ccxt\n",
            "  Downloading ccxt-1.72.72-py2.py3-none-any.whl (2.4 MB)\n",
            "\u001b[K     |████████████████████████████████| 2.4 MB 4.8 MB/s \n",
            "\u001b[?25hCollecting aiohttp>=3.8\n",
            "  Downloading aiohttp-3.8.1-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (1.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.1 MB 27.7 MB/s \n",
            "\u001b[?25hCollecting yarl==1.7.2\n",
            "  Downloading yarl-1.7.2-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (271 kB)\n",
            "\u001b[K     |████████████████████████████████| 271 kB 33.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: setuptools>=38.5.1 in /usr/local/lib/python3.7/dist-packages (from ccxt) (57.4.0)\n",
            "Collecting cryptography>=2.6.1\n",
            "  Downloading cryptography-36.0.1-cp36-abi3-manylinux_2_24_x86_64.whl (3.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.6 MB 42.1 MB/s \n",
            "\u001b[?25hCollecting aiodns>=1.1.1\n",
            "  Downloading aiodns-3.0.0-py3-none-any.whl (5.0 kB)\n",
            "Requirement already satisfied: certifi>=2018.1.18 in /usr/local/lib/python3.7/dist-packages (from ccxt) (2021.10.8)\n",
            "Requirement already satisfied: requests>=2.18.4 in /usr/local/lib/python3.7/dist-packages (from ccxt) (2.23.0)\n",
            "Collecting multidict>=4.0\n",
            "  Downloading multidict-6.0.2-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (94 kB)\n",
            "\u001b[K     |████████████████████████████████| 94 kB 3.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: typing-extensions>=3.7.4 in /usr/local/lib/python3.7/dist-packages (from yarl==1.7.2->ccxt) (3.10.0.2)\n",
            "Requirement already satisfied: idna>=2.0 in /usr/local/lib/python3.7/dist-packages (from yarl==1.7.2->ccxt) (2.10)\n",
            "Collecting pycares>=4.0.0\n",
            "  Downloading pycares-4.1.2-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (291 kB)\n",
            "\u001b[K     |████████████████████████████████| 291 kB 35.4 MB/s \n",
            "\u001b[?25hCollecting aiosignal>=1.1.2\n",
            "  Downloading aiosignal-1.2.0-py3-none-any.whl (8.2 kB)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp>=3.8->ccxt) (21.4.0)\n",
            "Collecting frozenlist>=1.1.1\n",
            "  Downloading frozenlist-1.3.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (144 kB)\n",
            "\u001b[K     |████████████████████████████████| 144 kB 44.1 MB/s \n",
            "\u001b[?25hCollecting asynctest==0.13.0\n",
            "  Downloading asynctest-0.13.0-py3-none-any.whl (26 kB)\n",
            "Requirement already satisfied: charset-normalizer<3.0,>=2.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp>=3.8->ccxt) (2.0.11)\n",
            "Collecting async-timeout<5.0,>=4.0.0a3\n",
            "  Downloading async_timeout-4.0.2-py3-none-any.whl (5.8 kB)\n",
            "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.7/dist-packages (from cryptography>=2.6.1->ccxt) (1.15.0)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.7/dist-packages (from cffi>=1.12->cryptography>=2.6.1->ccxt) (2.21)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.18.4->ccxt) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.18.4->ccxt) (3.0.4)\n",
            "Installing collected packages: multidict, frozenlist, yarl, pycares, asynctest, async-timeout, aiosignal, cryptography, aiohttp, aiodns, ccxt\n",
            "Successfully installed aiodns-3.0.0 aiohttp-3.8.1 aiosignal-1.2.0 async-timeout-4.0.2 asynctest-0.13.0 ccxt-1.72.72 cryptography-36.0.1 frozenlist-1.3.0 multidict-6.0.2 pycares-4.1.2 yarl-1.7.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "config = {\n",
        "    \"data\": {\n",
        "        \"window_size\": 14,\n",
        "        \"train_split_size\": 0.5,\n",
        "        \"val_split_size\": 0.2,\n",
        "        \"thres_frac\": 1.2,\n",
        "        \"change_time\": 3\n",
        "    }, \n",
        "    \"model\": {\n",
        "        \"input_size\": 2, # price, volume\n",
        "        \"num_lstm_layers\": 1,\n",
        "        \"hidden_size\": 64,\n",
        "        \"num_classes\" : 3,\n",
        "        \"dropout\": 0.2,\n",
        "    },\n",
        "    \"training\": {\n",
        "        \"device\": \"cpu\", # \"cuda\" or \"cpu\"\n",
        "        \"batch_size\": 32,\n",
        "        \"epoch\": 10,\n",
        "        \"learning_rate\": 0.01,\n",
        "        \"scheduler_step_size\": 40,\n",
        "    }\n",
        "}"
      ],
      "metadata": {
        "id": "tul_QAEsT60h"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. Data Preparation"
      ],
      "metadata": {
        "id": "WCfdy4fybvmz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "(1)Fetch Data"
      ],
      "metadata": {
        "id": "u_LL3glyhyZ1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "binance = ccxt.binance()\n",
        "fetch_num = 500\n",
        "timeframe = '1d'\n",
        "ticker = binance.fetch_ohlcv(\"BTC/USDT\", timeframe, limit=fetch_num)\n",
        "startfrom = ticker[0][0]\n",
        "class ohlcv:\n",
        "  def __init__(self, index):\n",
        "    self.index = index\n",
        "    since = startfrom - 1800000000 * 24 * self.index\n",
        "    ohlcv = binance.fetch_ohlcv(\"BTC/USDT\", timeframe, since=since, limit=fetch_num)\n",
        "    self.df = pd.DataFrame(ohlcv, columns=['datetime', 'open', 'high', 'low', 'close', 'volume'])\n",
        "\n",
        "ohlcv_df = pd.concat([ohlcv(2).df, ohlcv(1).df, ohlcv(0).df])\n",
        "ohlcv_df = ohlcv_df.drop_duplicates(['datetime'])\n",
        "ohlcv_df = ohlcv_df.drop(ohlcv_df.loc[ohlcv_df['close']==0].index)\n",
        "ohlcv_df = ohlcv_df.drop(ohlcv_df.loc[ohlcv_df['volume']==0].index)\n",
        "ohlcv_df['p_change'] = ohlcv_df['close'] - ohlcv_df['close'].shift(1)\n",
        "ohlcv_df['p_changerate'] = np.log(ohlcv_df['close']/ ohlcv_df['close'].shift(1))\n",
        "ohlcv_df['v_changerate'] = np.log(ohlcv_df['volume']/ ohlcv_df['volume'].shift(1))\n",
        "ohlcv_df = ohlcv_df.drop([0])\n",
        "ohlcv_df['datetime'] = pd.to_datetime(ohlcv_df['datetime'], unit='ms')\n",
        "datetime = ohlcv_df['datetime'].to_numpy()\n",
        "ohlcv_df.set_index('datetime', inplace=True)\n",
        "ohlcv_df.to_excel(\"ohlcv3.xlsx\")\n",
        "print(ohlcv_df.describe())\n",
        "p_change = ohlcv_df['p_change'].to_numpy()\n",
        "p_changerate = ohlcv_df['p_changerate'].to_numpy()\n",
        "v_changerate = ohlcv_df['v_changerate'].to_numpy()\n",
        "print(p_change.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SD2GJYpEh2Dv",
        "outputId": "7eb28c6a-baf4-4548-a77e-e2479aacf2f3"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "               open          high  ...  p_changerate  v_changerate\n",
            "count   1497.000000   1497.000000  ...   1497.000000   1497.000000\n",
            "mean   18993.546981  19541.320929  ...      0.000629     -0.001065\n",
            "std    17931.469431  18449.260344  ...      0.040926      0.397943\n",
            "min     3211.710000   3276.500000  ...     -0.502607     -3.687514\n",
            "25%     7131.590000   7340.000000  ...     -0.016147     -0.246804\n",
            "50%     9575.000000   9759.820000  ...      0.001501     -0.029903\n",
            "75%    33502.870000  34717.270000  ...      0.018675      0.242643\n",
            "max    67525.820000  69000.000000  ...      0.178449      2.599870\n",
            "\n",
            "[8 rows x 8 columns]\n",
            "(1497,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 새 섹션"
      ],
      "metadata": {
        "id": "cGiBMNxCB4pY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "(2)Data into train, test data"
      ],
      "metadata": {
        "id": "SLIfYv0N7KF8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# normalize\n",
        "def normalize(x):\n",
        "  mu = np.average(x)\n",
        "  sd = np.std(x)\n",
        "  normalized_x = (x - mu) / sd\n",
        "  print(mu)\n",
        "  print(sd)\n",
        "  return normalized_x\n",
        "\n",
        "p_changerate = normalize(p_changerate)\n",
        "v_changerate = normalize(v_changerate)\n",
        "\n",
        "#perform windowing\n",
        "def prepare_data_x(p_changerate, v_changerate, window_size):\n",
        "   n_row = p_changerate.shape[0] - window_size + 1\n",
        "   x1 = np.lib.stride_tricks.as_strided(p_changerate, shape=(n_row,window_size), strides=(p_changerate.strides[0],p_changerate.strides[0]))\n",
        "   x2 = np.lib.stride_tricks.as_strided(v_changerate, shape=(n_row,window_size), strides=(v_changerate.strides[0],v_changerate.strides[0]))\n",
        "   li = [-i for i in range(1, config[\"data\"][\"change_time\"]+1)]\n",
        "   x1 = np.delete(x1, li, axis=0)\n",
        "   x2 = np.delete(x2, li, axis=0)\n",
        "   x = np.dstack([x1, x2])\n",
        "   x = x.astype(np.float32)\n",
        "   return x\n",
        "\n",
        "#calculate deviation by window\n",
        "def prepare_data_y(p_change, window_size, fraction):\n",
        "  n_row = p_change.shape[0] - window_size + 1\n",
        "  windowed_change = np.lib.stride_tricks.as_strided(p_change, shape=(n_row,window_size), strides=(p_change.strides[0],p_change.strides[0]))\n",
        "  threshold = fraction * np.std(windowed_change, axis=1)\n",
        "  print(threshold.shape)\n",
        "  yn = np.zeros(n_row-1)\n",
        "  for i in range(n_row-1):\n",
        "    li = p_change[window_size+i:window_size+i+ config[\"data\"][\"change_time\"]]\n",
        "    change = sum(li)\n",
        "    if change >= threshold[i]:\n",
        "      yn[i] = 1\n",
        "    elif change <= -threshold[i]:\n",
        "      yn[i] = 2\n",
        "  return yn\n",
        "\n",
        "\n",
        "x = prepare_data_x(p_changerate, v_changerate, config[\"data\"][\"window_size\"])\n",
        "y = prepare_data_y(p_change, config[\"data\"][\"window_size\"],  config[\"data\"][\"thres_frac\"])\n",
        "\n",
        "# split dataset\n",
        "def split_dataset(x, y, shuffle=False):\n",
        "  split_index = int(x.shape[0]* config[\"data\"][\"train_split_size\"])\n",
        "  split_index2 = int((x.shape[0]* (config[\"data\"][\"train_split_size\"]+config[\"data\"][\"val_split_size\"])\n",
        "  x_train = x[:split_index]\n",
        "  x_val = x[split_index: split_index2]\n",
        "  x_test = x[split_index2:]\n",
        "  y_train = y[:split_index]\n",
        "  y_val = y[split_index: split_index2]\n",
        "  y_test = y[split_index2:]\n",
        "  return x_train, y_train, x_val, y_val, x_test, y_test\n",
        "\n",
        "x_train = split_dataset(x, y)[0]\n",
        "y_train = split_dataset(x, y)[1]\n",
        "x_val = split_dataset(x, y)[2]\n",
        "y_ val = split_dataset(x, y)[3]\n",
        "x_test = split_dataset(x, y)[4]\n",
        "y_test = split_dataset(x, y)[5]\n",
        "\n",
        "class TimeSeriesDataset(Dataset):\n",
        "    def __init__(self, x, y):\n",
        "        self.x = x.astype(np.float32)\n",
        "        self.y = y.astype(np.int64)\n",
        "        \n",
        "    def __len__(self):\n",
        "        return len(self.x)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return (self.x[idx], self.y[idx])\n",
        "\n",
        "dataset_train = TimeSeriesDataset(x_train, y_train)\n",
        "dataset_val = TimeSeriesDataset(x_val, y_val)\n",
        "dataset_test = TimeSeriesDataset(x_test, y_test)\n",
        "\n",
        "train_loader = DataLoader(dataset_train, batch_size=config[\"training\"][\"batch_size\"], shuffle=False)\n",
        "val_loader = DataLoader(dataset_val, batch_size=config[\"training\"][\"batch_size\"], shuffle=False)\n",
        "test_loader = DataLoader(dataset_test, batch_size=config[\"training\"][\"batch_size\"], shuffle=False)\n",
        "\n",
        "count = 0\n",
        "for (x_train, y_train) in train_loader:\n",
        "  count += 1\n",
        "print(count)\n",
        "for (x_train, y_train) in train_loader: \n",
        "  print(x_train.size(), x_train.type())\n",
        "  print(y_train.size(), y_train.type())\n",
        "  break"
      ],
      "metadata": {
        "id": "NGNAHCnv7Xxd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "15e91783-5e48-444b-ed38-679bac456310"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.0006292588035143804\n",
            "0.040911879727100066\n",
            "-0.001065308288639101\n",
            "0.39781017599999197\n",
            "(1484,)\n",
            "33\n",
            "torch.Size([32, 14, 2]) torch.FloatTensor\n",
            "torch.Size([32]) torch.LongTensor\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. Defining Model"
      ],
      "metadata": {
        "id": "vyFETWkdIkeV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class LSTMModel(nn.Module):\n",
        "  def __init__(self, num_classes, input_size, hidden_size, num_layers):\n",
        "    super().__init__()\n",
        "    self.num_classes = num_classes #number of classes\n",
        "    self.num_layers = num_layers #number of layers\n",
        "    self.input_size = input_size #input size\n",
        "    self.hidden_size = hidden_size #hidden state\n",
        "    self.lstm = nn.LSTM(input_size=input_size, hidden_size=hidden_size,\n",
        "                          num_layers=num_layers, batch_first=True) #lstm1\n",
        "    self.fc1 = nn.Linear(hidden_size, 512) #fully connected 1\n",
        "    self.fc2 = nn.Linear(512, 128) #fully connected 2\n",
        "    self.fc3 = nn.Linear(128, num_classes) #fully connected last layer\n",
        "    self.relu = nn.ReLU()\n",
        "    self.batch_norm1 = nn.BatchNorm1d(hidden_size)\n",
        "    self.batch_norm2 = nn.BatchNorm1d(512)\n",
        "    self.batch_norm3 = nn.BatchNorm1d(128)\n",
        "    self.dropout_prob = 0.2\n",
        "  \n",
        "  def forward(self,x):\n",
        "    # Propagate input through LSTM\n",
        "    output, (hn, cn) = self.lstm(x) #lstm with input, hidden, and internal state\n",
        "    x = hn.view(-1, self.hidden_size) #reshaping the data for Dense layer next\n",
        "    x = self.batch_norm1(x)\n",
        "    x = self.fc1(x) #first Dense\n",
        "    x = self.batch_norm2(x)\n",
        "    x = self.relu(x) #relu\n",
        "    x = F.dropout(x, training=self.training, p=self.dropout_prob)\n",
        "    x = self.fc2(x) #second Dense\n",
        "    x = self.batch_norm3(x)\n",
        "    x = self.relu(x) #relu\n",
        "    x = F.dropout(x, training=self.training, p=self.dropout_prob)\n",
        "    x = self.fc3(x) #Final Output\n",
        "    x = F.log_softmax(x)\n",
        "    return x\n",
        "\n",
        "model = LSTMModel(num_classes=config[\"model\"][\"num_classes\"], input_size=config[\"model\"][\"input_size\"], hidden_size=config[\"model\"][\"hidden_size\"], num_layers=config[\"model\"][\"num_lstm_layers\"])\n",
        "model = model.to(config[\"training\"][\"device\"])"
      ],
      "metadata": {
        "id": "G7aAqzloInl8"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. Optimizer, Objective Function 설정하기"
      ],
      "metadata": {
        "id": "4qnz2nMUpB5p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = optim.Adam(model.parameters(), lr=config[\"training\"][\"learning_rate\"])\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "def weight_init(m):\n",
        "  if isinstance(m, nn.Linear):\n",
        "    nn.init.kaiming_uniform(m.weight.data)\n",
        "model.apply(weight_init)"
      ],
      "metadata": {
        "id": "bK14LBmApNgs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6b1eef37-cc4b-4c7e-cd50-00f4c24eae1b"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:5: UserWarning: nn.init.kaiming_uniform is now deprecated in favor of nn.init.kaiming_uniform_.\n",
            "  \"\"\"\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LSTMModel(\n",
              "  (lstm): LSTM(2, 64, batch_first=True)\n",
              "  (fc1): Linear(in_features=64, out_features=512, bias=True)\n",
              "  (fc2): Linear(in_features=512, out_features=128, bias=True)\n",
              "  (fc3): Linear(in_features=128, out_features=3, bias=True)\n",
              "  (relu): ReLU()\n",
              "  (batch_norm1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  (batch_norm2): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  (batch_norm3): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "5. Train model"
      ],
      "metadata": {
        "id": "i38xX6mdpk9Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train(model, train_loader, optimizer, log_interval = 20):\n",
        "  model.train()\n",
        "  for idx, (x, y) in enumerate(train_loader):\n",
        "    x = x.to(config[\"training\"][\"device\"])\n",
        "    y = y.to(config[\"training\"][\"device\"])\n",
        "    optimizer.zero_grad()\n",
        "    out = model(x)\n",
        "    loss = criterion(out, y)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    if idx % log_interval == 0:\n",
        "      print(\"Train Epoch: {} [{}/{} ({:.0f}%)] Train Loss: {:.6f}\".format(epoch, idx * len(x),\n",
        "      len(train_loader.dataset), 100. * idx / len(train_loader), loss.item()))"
      ],
      "metadata": {
        "id": "F2aO4Muapnls"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "6. Evaluate Model"
      ],
      "metadata": {
        "id": "b_-0qRXWRzHV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate(model, test_loader):\n",
        "  model.eval()\n",
        "  test_loss = 0\n",
        "  correct = 0\n",
        "\n",
        "  with torch.no_grad():\n",
        "    for x, y in test_loader:\n",
        "      x = x.to(config[\"training\"][\"device\"])\n",
        "      y = y.to(config[\"training\"][\"device\"])\n",
        "      out = model(x)\n",
        "      test_loss += criterion(out, y).item()\n",
        "      prediction = out.max(1, True)[1]\n",
        "      correct += prediction.eq(y.view_as(prediction)).sum().item()\n",
        "  \n",
        "  test_loss /= len(test_loader.dataset)\n",
        "  test_accuracy = 100. * correct / len(test_loader.dataset)\n",
        "  return test_loss, test_accuracy, prediction, out"
      ],
      "metadata": {
        "id": "aGDJDH-bR3V-"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "7. 학습 진행 및 평가"
      ],
      "metadata": {
        "id": "LEC1sTssUBHT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for epoch in range(1, config[\"training\"][\"epoch\"] + 1):\n",
        "  train(model, train_loader, optimizer, log_interval=10)\n",
        "  test_loss, test_accuracy, prediction, out = evaluate(model, val_loader)\n",
        "  print(\"[EPOCH: {}], Test Loss: {:.4f}, Test Accuracy: {:.2f} %\".format(epoch, test_loss, test_accuracy))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sTpRqo8dUiDV",
        "outputId": "46acc478-a12a-4e27-d0e0-e43be90949b2"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:33: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Epoch: 1 [0/1036 (0%)] Train Loss: 0.802172\n",
            "Train Epoch: 1 [320/1036 (30%)] Train Loss: 0.989690\n",
            "Train Epoch: 1 [640/1036 (61%)] Train Loss: 1.185845\n",
            "Train Epoch: 1 [960/1036 (91%)] Train Loss: 0.726407\n",
            "[EPOCH: 1], Test Loss: 0.0380, Test Accuracy: 47.42 %\n",
            "Train Epoch: 2 [0/1036 (0%)] Train Loss: 0.779618\n",
            "Train Epoch: 2 [320/1036 (30%)] Train Loss: 0.826282\n",
            "Train Epoch: 2 [640/1036 (61%)] Train Loss: 1.031748\n",
            "Train Epoch: 2 [960/1036 (91%)] Train Loss: 0.690479\n",
            "[EPOCH: 2], Test Loss: 0.0472, Test Accuracy: 45.39 %\n",
            "Train Epoch: 3 [0/1036 (0%)] Train Loss: 0.734260\n",
            "Train Epoch: 3 [320/1036 (30%)] Train Loss: 0.882606\n",
            "Train Epoch: 3 [640/1036 (61%)] Train Loss: 0.917519\n",
            "Train Epoch: 3 [960/1036 (91%)] Train Loss: 0.675098\n",
            "[EPOCH: 3], Test Loss: 0.0380, Test Accuracy: 48.99 %\n",
            "Train Epoch: 4 [0/1036 (0%)] Train Loss: 0.750791\n",
            "Train Epoch: 4 [320/1036 (30%)] Train Loss: 0.765672\n",
            "Train Epoch: 4 [640/1036 (61%)] Train Loss: 0.975580\n",
            "Train Epoch: 4 [960/1036 (91%)] Train Loss: 0.616992\n",
            "[EPOCH: 4], Test Loss: 0.0456, Test Accuracy: 46.97 %\n",
            "Train Epoch: 5 [0/1036 (0%)] Train Loss: 0.586876\n",
            "Train Epoch: 5 [320/1036 (30%)] Train Loss: 0.834353\n",
            "Train Epoch: 5 [640/1036 (61%)] Train Loss: 0.930068\n",
            "Train Epoch: 5 [960/1036 (91%)] Train Loss: 0.583307\n",
            "[EPOCH: 5], Test Loss: 0.0439, Test Accuracy: 45.84 %\n",
            "Train Epoch: 6 [0/1036 (0%)] Train Loss: 0.502888\n",
            "Train Epoch: 6 [320/1036 (30%)] Train Loss: 0.744978\n",
            "Train Epoch: 6 [640/1036 (61%)] Train Loss: 0.814505\n",
            "Train Epoch: 6 [960/1036 (91%)] Train Loss: 0.572969\n",
            "[EPOCH: 6], Test Loss: 0.0427, Test Accuracy: 42.92 %\n",
            "Train Epoch: 7 [0/1036 (0%)] Train Loss: 0.362082\n",
            "Train Epoch: 7 [320/1036 (30%)] Train Loss: 0.738317\n",
            "Train Epoch: 7 [640/1036 (61%)] Train Loss: 0.927027\n",
            "Train Epoch: 7 [960/1036 (91%)] Train Loss: 0.521917\n",
            "[EPOCH: 7], Test Loss: 0.0391, Test Accuracy: 38.65 %\n",
            "Train Epoch: 8 [0/1036 (0%)] Train Loss: 0.475104\n",
            "Train Epoch: 8 [320/1036 (30%)] Train Loss: 0.979801\n",
            "Train Epoch: 8 [640/1036 (61%)] Train Loss: 0.852478\n",
            "Train Epoch: 8 [960/1036 (91%)] Train Loss: 0.572057\n",
            "[EPOCH: 8], Test Loss: 0.0521, Test Accuracy: 43.15 %\n",
            "Train Epoch: 9 [0/1036 (0%)] Train Loss: 0.396294\n",
            "Train Epoch: 9 [320/1036 (30%)] Train Loss: 0.737637\n",
            "Train Epoch: 9 [640/1036 (61%)] Train Loss: 0.663089\n",
            "Train Epoch: 9 [960/1036 (91%)] Train Loss: 0.586110\n",
            "[EPOCH: 9], Test Loss: 0.0607, Test Accuracy: 45.84 %\n",
            "Train Epoch: 10 [0/1036 (0%)] Train Loss: 0.366601\n",
            "Train Epoch: 10 [320/1036 (30%)] Train Loss: 0.625150\n",
            "Train Epoch: 10 [640/1036 (61%)] Train Loss: 0.723416\n",
            "Train Epoch: 10 [960/1036 (91%)] Train Loss: 0.618906\n",
            "[EPOCH: 10], Test Loss: 0.0594, Test Accuracy: 40.90 %\n"
          ]
        }
      ]
    }
  ]
}